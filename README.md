# Attention Eval

A collection of experiments for efficient attention.

Objective:
* Find a novel way to reduce time/space complexity of transformer architectures
* Create a simple and effective attention module to improve performance on a variety of downstream tasks

Current efforts:
* Exploring token merging (ToMe)
* Exploring masked variational autoencoder (MAE)
* Exploring flash attention
* Exploring "simplified transformer blocks"
* Exploring neighborhood attention

Other interests:
* What attention "tricks" do you have to pull for high resolution samples?
* Layer wise learning rate decay