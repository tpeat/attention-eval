{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc89cfcd-9fc1-4230-b321-f666c44f840d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from util.helpers import vis, get_all_frames_labels\n",
    "from mae import MaskedAutoencoderViT, mae_vit_base_patch16_dec512d8b\n",
    "\n",
    "os.chdir('../aot-benchmark')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27fea275-2d54-402c-821d-ff3c4a3a31ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b997c757-cbbf-45e5-b5f9-4ad54e9983e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim for norm: 96\n",
      "Dim for norm: 96\n",
      "Dim for norm: 96\n",
      "Dim for norm: 192\n",
      "Dim for norm: 192\n",
      "Dim for norm: 192\n",
      "Dim for norm: 384\n",
      "Dim for norm: 384\n",
      "Dim for norm: 384\n",
      "Dim for norm: 768\n",
      "Dim for norm: 768\n",
      "Dim for norm: 768\n",
      "Dim for norm: 768\n"
     ]
    }
   ],
   "source": [
    "model = MaskedAutoencoderViT(img_size=464, patch_size=16, in_chans=3,\n",
    "                 embed_dim=[96, 96, 96, 192, 192, 192, 384, 384, 384, 768, 768, 768], depth=12, num_heads=16,\n",
    "                 decoder_embed_dim=768, decoder_depth=1, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.Identity, norm_pix_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c62285df-25f9-4645-aa55-28c083662846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dfd9ae-a0f0-411c-9642-07867421fc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = mae_vit_base_patch16_dec512d8b(img_size=465).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e97fc1e-1a1e-41a8-8aef-2566ac53ebc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "./result/tristan_test_AOTT/PRE/ckpt\n",
      "*********************************************************************************************************************************\n",
      "Video Num: 23 X 1\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "import importlib\n",
    "from dataloaders.train_datasets import AOT_Train\n",
    "from torch.utils.data import DataLoader\n",
    "import dataloaders.video_transforms as tr\n",
    "from torchvision import transforms\n",
    "\n",
    "stage = 'pre'\n",
    "exp_name = \"tristan_test\"\n",
    "model_name = \"aott\"\n",
    "engine_config = importlib.import_module('configs.' + stage)\n",
    "cfg = engine_config.EngineConfig(exp_name, model_name)\n",
    "\n",
    "cfg.DIST_START_GPU = 0  # default value\n",
    "cfg.TRAIN_GPUS = 1\n",
    "cfg.DATASETS = 'AOT'\n",
    "cfg.DIR_AOT = '/gv1/projects/AI_Surrogate/dev/tristan/aot-benchmark/datasets/AOT'\n",
    "cfg.PRETRAIN_MODEL = \"pretrain_model/mobilenet_v2-b0353104.pth\"\n",
    "cfg.DIST_ENABLE = False\n",
    "cfg.DATA_WORKERS = 2\n",
    "cfg.TRAIN_BATCH_SIZE = 1\n",
    "cfg.DATA_RANDOMCROP=464\n",
    "\n",
    "cfg.enable_prev_frame = True\n",
    "train_sampler = None\n",
    "\n",
    "composed_transforms = transforms.Compose([\n",
    "                tr.RandomScale(cfg.DATA_MIN_SCALE_FACTOR,\n",
    "                               cfg.DATA_MAX_SCALE_FACTOR,\n",
    "                               cfg.DATA_SHORT_EDGE_LEN),\n",
    "                tr.BalancedRandomCrop(cfg.DATA_RANDOMCROP,\n",
    "                                      max_obj_num=cfg.MODEL_MAX_OBJ_NUM),\n",
    "                tr.RandomHorizontalFlip(cfg.DATA_RANDOMFLIP),\n",
    "                tr.Resize(cfg.DATA_RANDOMCROP, use_padding=True),\n",
    "                tr.ToTensor()])\n",
    "\n",
    "train_dataset = AOT_Train(\n",
    "                root=cfg.DIR_AOT,\n",
    "                transform=composed_transforms,\n",
    "                seq_len=cfg.DATA_SEQ_LEN,\n",
    "                rand_gap=cfg.DATA_RANDOM_GAP_AOT,\n",
    "                rand_reverse=cfg.DATA_RANDOM_REVERSE_SEQ,\n",
    "                merge_prob=cfg.DATA_DYNAMIC_MERGE_PROB,\n",
    "                enable_prev_frame=cfg.enable_prev_frame,\n",
    "                max_obj_n=cfg.MODEL_MAX_OBJ_NUM)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "               batch_size=int(cfg.TRAIN_BATCH_SIZE /\n",
    "                              cfg.TRAIN_GPUS),\n",
    "               shuffle=False if cfg.DIST_ENABLE else True,\n",
    "               num_workers=cfg.DATA_WORKERS,\n",
    "               pin_memory=True,\n",
    "               sampler=train_sampler,\n",
    "               drop_last=True,\n",
    "               prefetch_factor=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c843998-e1c6-41db-bd97-a691ad1b2891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for frame_idx, sample in enumerate(train_loader):\n",
    "    all_frames, all_labels, bs, obj_nums = get_all_frames_labels(sample, 0) # obj nums is the classes\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5b437e8-d978-40a3-9046-1b3a1fb85c14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 464, 464])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68dd370c-cd2a-408c-85e6-d96954d55379",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb79d36-cd6a-4c94-a928-e7ad1f1ebe5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db2d4669-f614-4dc2-ae58-decf8c073083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54578a0f-d428-4743-9b7b-1c2d936c3410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = VisionTransformer(\n",
    "    img_size=465,\n",
    "    patch_size=15,\n",
    "    num_classes=10,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bbcc58d-4712-4309-8067-f9fc0afcd06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0541,  0.3998,  1.0502, -0.7676,  0.0494, -0.7532,  0.9203, -0.5654,\n",
       "         -0.3561,  0.3753],\n",
       "        [ 0.0354,  0.3576,  1.0069, -0.7382,  0.0290, -0.8259,  0.8893, -0.5758,\n",
       "         -0.2815,  0.4169],\n",
       "        [ 0.0296,  0.3585,  1.0130, -0.7445,  0.0295, -0.8259,  0.8898, -0.5791,\n",
       "         -0.2739,  0.4148],\n",
       "        [ 0.0368,  0.3589,  1.0240, -0.7475,  0.0292, -0.8081,  0.8945, -0.5773,\n",
       "         -0.2934,  0.4151],\n",
       "        [ 0.0444,  0.3634,  1.0280, -0.7481,  0.0295, -0.7995,  0.8929, -0.5710,\n",
       "         -0.2928,  0.4132],\n",
       "        [ 0.0405,  0.3898,  1.0462, -0.7587,  0.0332, -0.7734,  0.8959, -0.5792,\n",
       "         -0.3250,  0.3978]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11ab51f2-04eb-4210-aadf-d8498998b4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def func(module, name):\n",
    "    print(\"found:\", name)\n",
    "\n",
    "def named_apply(fn: Callable, module: nn.Module, name='', depth_first=True, include_root=False) -> nn.Module:\n",
    "    if not depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_name = '.'.join((name, child_name)) if name else child_name\n",
    "        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n",
    "    if depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af974941-c928-4cc4-8712-2258910e1222",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found: patch_embed.proj\n",
      "found: patch_embed.norm\n",
      "found: patch_embed\n",
      "found: blocks.0.norm1\n",
      "found: blocks.0.attn.qkv\n",
      "found: blocks.0.attn.attn_drop\n",
      "found: blocks.0.attn.proj\n",
      "found: blocks.0.attn.proj_drop\n",
      "found: blocks.0.attn\n",
      "found: blocks.0.ls1\n",
      "found: blocks.0.drop_path1\n",
      "found: blocks.0.norm2\n",
      "found: blocks.0.mlp.fc1\n",
      "found: blocks.0.mlp.act\n",
      "found: blocks.0.mlp.drop1\n",
      "found: blocks.0.mlp.fc2\n",
      "found: blocks.0.mlp.drop2\n",
      "found: blocks.0.mlp\n",
      "found: blocks.0.ls2\n",
      "found: blocks.0.drop_path2\n",
      "found: blocks.0\n",
      "found: blocks\n",
      "found: norm\n",
      "found: decoder_embed\n",
      "found: decoder_blocks.0.norm1\n",
      "found: decoder_blocks.0.attn.qkv\n",
      "found: decoder_blocks.0.attn.attn_drop\n",
      "found: decoder_blocks.0.attn.proj\n",
      "found: decoder_blocks.0.attn.proj_drop\n",
      "found: decoder_blocks.0.attn\n",
      "found: decoder_blocks.0.ls1\n",
      "found: decoder_blocks.0.drop_path1\n",
      "found: decoder_blocks.0.norm2\n",
      "found: decoder_blocks.0.mlp.fc1\n",
      "found: decoder_blocks.0.mlp.act\n",
      "found: decoder_blocks.0.mlp.drop1\n",
      "found: decoder_blocks.0.mlp.fc2\n",
      "found: decoder_blocks.0.mlp.drop2\n",
      "found: decoder_blocks.0.mlp\n",
      "found: decoder_blocks.0.ls2\n",
      "found: decoder_blocks.0.drop_path2\n",
      "found: decoder_blocks.0\n",
      "found: decoder_blocks\n",
      "found: decoder_norm\n",
      "found: decoder_pred\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskedAutoencoderViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(15, 15), stride=(15, 15))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (decoder_embed): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (decoder_pred): Linear(in_features=512, out_features=675, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_apply(func, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b0e1546-6212-4609-a9ef-52468b35c7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.layers import resample_patch_embed, resample_abs_pos_embed\n",
    "\n",
    "def load_network(net, pretrained_dir, gpu):\n",
    "    antialias = True\n",
    "    pretrained = torch.load(pretrained_dir,\n",
    "                            map_location=device)\n",
    "    print(pretrained.keys())\n",
    "    if 'state_dict' in pretrained.keys():\n",
    "        pretrained_dict = pretrained['state_dict']\n",
    "    elif 'model_state' in pretrained.keys():\n",
    "        pretrained_dict = pretrained['model_state']\n",
    "    else:\n",
    "        pretrained_dict = pretrained\n",
    "    model_dict = net.state_dict()\n",
    "    pretrained_dict_update = {}\n",
    "    pretrained_dict_remove = []\n",
    "    for k, v in pretrained_dict.items():\n",
    "         # move to device\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "            if len(v.shape) < 4:\n",
    "                # For old models that I trained prior to conv based patchification\n",
    "                O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "                v = v.reshape(O, -1, H, W)\n",
    "                if v.shape[-1] != W or v.shape[-2] != H:\n",
    "                    v = resample_patch_embed(\n",
    "                        v,\n",
    "                        (H, W),\n",
    "                        interpolation='bicubic',\n",
    "                        antialias=antialias,\n",
    "                        verbose=True,\n",
    "                    )\n",
    "            elif v.shape != model.patch_embed.proj.weight.shape:\n",
    "                print(\"interpolating v\")\n",
    "                # send to cpu and then bring back after\n",
    "                v = v.to('cpu')\n",
    "                O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "                v = resample_patch_embed(\n",
    "                        v,\n",
    "                        (H, W),\n",
    "                        interpolation='bicubic',\n",
    "                        antialias=antialias,\n",
    "                        verbose=True,\n",
    "                    )\n",
    "                v = v.to(device)\n",
    "            elif k == 'pos_embed' and v.shape[1] != model.pos_embed.shape[1]:\n",
    "                # To resize pos embedding when using model at different size from pretrained weights\n",
    "                num_prefix_tokens = 0 if getattr(model, 'no_embed_class', False) else getattr(model, 'num_prefix_tokens', 1)\n",
    "                v = resample_abs_pos_embed(\n",
    "                    v,\n",
    "                    new_size=model.patch_embed.grid_size,\n",
    "                    num_prefix_tokens=num_prefix_tokens,\n",
    "                    interpolation=interpolation,\n",
    "                    antialias=antialias,\n",
    "                    verbose=True,\n",
    "                )\n",
    "            if k in model_dict:\n",
    "                pretrained_dict_update[k] = v\n",
    "            elif k[:7] == 'module.':\n",
    "                if k[7:] in model_dict:\n",
    "                    pretrained_dict_update[k[7:]] = v\n",
    "            else:\n",
    "                print(\"removing:\", k)\n",
    "                pretrained_dict_remove.append(k)\n",
    "    print(pretrained_dict_update.keys())\n",
    "    model_dict.update(pretrained_dict_update)\n",
    "    net.load_state_dict(model_dict, strict=False)\n",
    "    del (pretrained)\n",
    "    return net.cuda(gpu), pretrained_dict_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "660580d4-84cb-41e7-8aa6-2bceda7f0221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'model_state', 'optimizer_state', 'scaler_state'])\n",
      "interpolating v\n",
      "dict_keys(['patch_embed.proj.weight'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MaskedAutoencoderViT(\n",
       "   (patch_embed): PatchEmbed(\n",
       "     (proj): Conv2d(3, 96, kernel_size=(16, 16), stride=(16, 16))\n",
       "     (norm): Identity()\n",
       "   )\n",
       "   (blocks): ModuleList(\n",
       "     (0-2): 3 x Block(\n",
       "       (norm1): Identity()\n",
       "       (attn): Attention(\n",
       "         (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "         (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "         (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "         (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls1): Identity()\n",
       "       (drop_path1): Identity()\n",
       "       (norm2): Identity()\n",
       "       (mlp): Mlp(\n",
       "         (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "         (act): GELU(approximate='none')\n",
       "         (drop1): Dropout(p=0.0, inplace=False)\n",
       "         (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "         (drop2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls2): Identity()\n",
       "       (drop_path2): Identity()\n",
       "     )\n",
       "     (3-5): 3 x Block(\n",
       "       (norm1): Identity()\n",
       "       (attn): Attention(\n",
       "         (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "         (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "         (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "         (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls1): Identity()\n",
       "       (drop_path1): Identity()\n",
       "       (norm2): Identity()\n",
       "       (mlp): Mlp(\n",
       "         (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "         (act): GELU(approximate='none')\n",
       "         (drop1): Dropout(p=0.0, inplace=False)\n",
       "         (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "         (drop2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls2): Identity()\n",
       "       (drop_path2): Identity()\n",
       "     )\n",
       "     (6-8): 3 x Block(\n",
       "       (norm1): Identity()\n",
       "       (attn): Attention(\n",
       "         (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "         (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "         (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "         (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls1): Identity()\n",
       "       (drop_path1): Identity()\n",
       "       (norm2): Identity()\n",
       "       (mlp): Mlp(\n",
       "         (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (act): GELU(approximate='none')\n",
       "         (drop1): Dropout(p=0.0, inplace=False)\n",
       "         (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (drop2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls2): Identity()\n",
       "       (drop_path2): Identity()\n",
       "     )\n",
       "     (9-11): 3 x Block(\n",
       "       (norm1): Identity()\n",
       "       (attn): Attention(\n",
       "         (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "         (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "         (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls1): Identity()\n",
       "       (drop_path1): Identity()\n",
       "       (norm2): Identity()\n",
       "       (mlp): Mlp(\n",
       "         (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "         (act): GELU(approximate='none')\n",
       "         (drop1): Dropout(p=0.0, inplace=False)\n",
       "         (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "         (drop2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls2): Identity()\n",
       "       (drop_path2): Identity()\n",
       "     )\n",
       "   )\n",
       "   (norm): Identity()\n",
       "   (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "   (decoder_blocks): ModuleList(\n",
       "     (0): Block(\n",
       "       (norm1): Identity()\n",
       "       (attn): Attention(\n",
       "         (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "         (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "         (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "         (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls1): Identity()\n",
       "       (drop_path1): Identity()\n",
       "       (norm2): Identity()\n",
       "       (mlp): Mlp(\n",
       "         (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "         (act): GELU(approximate='none')\n",
       "         (drop1): Dropout(p=0.0, inplace=False)\n",
       "         (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         (drop2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (ls2): Identity()\n",
       "       (drop_path2): Identity()\n",
       "     )\n",
       "   )\n",
       "   (decoder_norm): Identity()\n",
       "   (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       " ),\n",
       " [])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_network(model, 'pretrain_models/hiera_base_224.pth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ee391-7a45-4f0f-abd2-8202f852cdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "936a15ce-a5d8-447d-a655-35c4f4c3f09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('pretrain_models/hiera_base_224.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49341689-f108-4a7b-abb9-9803ba144ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 3, 7, 7])\n",
      "blocks.0.norm1.weight torch.Size([96])\n",
      "blocks.0.norm1.bias torch.Size([96])\n",
      "blocks.0.attn.qkv.weight torch.Size([288, 96])\n",
      "blocks.0.attn.qkv.bias torch.Size([288])\n",
      "blocks.0.attn.proj.weight torch.Size([96, 96])\n",
      "blocks.0.attn.proj.bias torch.Size([96])\n",
      "blocks.0.norm2.weight torch.Size([96])\n",
      "blocks.0.norm2.bias torch.Size([96])\n",
      "blocks.0.mlp.fc1.weight torch.Size([384, 96])\n",
      "blocks.0.mlp.fc1.bias torch.Size([384])\n",
      "blocks.0.mlp.fc2.weight torch.Size([96, 384])\n",
      "blocks.0.mlp.fc2.bias torch.Size([96])\n",
      "blocks.1.norm1.weight torch.Size([96])\n",
      "blocks.1.norm1.bias torch.Size([96])\n",
      "blocks.1.attn.qkv.weight torch.Size([288, 96])\n",
      "blocks.1.attn.qkv.bias torch.Size([288])\n",
      "blocks.1.attn.proj.weight torch.Size([96, 96])\n",
      "blocks.1.attn.proj.bias torch.Size([96])\n",
      "blocks.1.norm2.weight torch.Size([96])\n",
      "blocks.1.norm2.bias torch.Size([96])\n",
      "blocks.1.mlp.fc1.weight torch.Size([384, 96])\n",
      "blocks.1.mlp.fc1.bias torch.Size([384])\n",
      "blocks.1.mlp.fc2.weight torch.Size([96, 384])\n",
      "blocks.1.mlp.fc2.bias torch.Size([96])\n",
      "blocks.2.norm1.weight torch.Size([96])\n",
      "blocks.2.norm1.bias torch.Size([96])\n",
      "blocks.2.attn.qkv.weight torch.Size([576, 96])\n",
      "blocks.2.attn.qkv.bias torch.Size([576])\n",
      "blocks.2.attn.proj.weight torch.Size([192, 192])\n",
      "blocks.2.attn.proj.bias torch.Size([192])\n",
      "blocks.2.norm2.weight torch.Size([192])\n",
      "blocks.2.norm2.bias torch.Size([192])\n",
      "blocks.2.mlp.fc1.weight torch.Size([768, 192])\n",
      "blocks.2.mlp.fc1.bias torch.Size([768])\n",
      "blocks.2.mlp.fc2.weight torch.Size([192, 768])\n",
      "blocks.2.mlp.fc2.bias torch.Size([192])\n",
      "blocks.2.proj.weight torch.Size([192, 96])\n",
      "blocks.2.proj.bias torch.Size([192])\n",
      "blocks.3.norm1.weight torch.Size([192])\n",
      "blocks.3.norm1.bias torch.Size([192])\n",
      "blocks.3.attn.qkv.weight torch.Size([576, 192])\n",
      "blocks.3.attn.qkv.bias torch.Size([576])\n",
      "blocks.3.attn.proj.weight torch.Size([192, 192])\n",
      "blocks.3.attn.proj.bias torch.Size([192])\n",
      "blocks.3.norm2.weight torch.Size([192])\n",
      "blocks.3.norm2.bias torch.Size([192])\n",
      "blocks.3.mlp.fc1.weight torch.Size([768, 192])\n",
      "blocks.3.mlp.fc1.bias torch.Size([768])\n",
      "blocks.3.mlp.fc2.weight torch.Size([192, 768])\n",
      "blocks.3.mlp.fc2.bias torch.Size([192])\n",
      "blocks.4.norm1.weight torch.Size([192])\n",
      "blocks.4.norm1.bias torch.Size([192])\n",
      "blocks.4.attn.qkv.weight torch.Size([576, 192])\n",
      "blocks.4.attn.qkv.bias torch.Size([576])\n",
      "blocks.4.attn.proj.weight torch.Size([192, 192])\n",
      "blocks.4.attn.proj.bias torch.Size([192])\n",
      "blocks.4.norm2.weight torch.Size([192])\n",
      "blocks.4.norm2.bias torch.Size([192])\n",
      "blocks.4.mlp.fc1.weight torch.Size([768, 192])\n",
      "blocks.4.mlp.fc1.bias torch.Size([768])\n",
      "blocks.4.mlp.fc2.weight torch.Size([192, 768])\n",
      "blocks.4.mlp.fc2.bias torch.Size([192])\n",
      "blocks.5.norm1.weight torch.Size([192])\n",
      "blocks.5.norm1.bias torch.Size([192])\n",
      "blocks.5.attn.qkv.weight torch.Size([1152, 192])\n",
      "blocks.5.attn.qkv.bias torch.Size([1152])\n",
      "blocks.5.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.5.attn.proj.bias torch.Size([384])\n",
      "blocks.5.norm2.weight torch.Size([384])\n",
      "blocks.5.norm2.bias torch.Size([384])\n",
      "blocks.5.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.5.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.5.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.5.mlp.fc2.bias torch.Size([384])\n",
      "blocks.5.proj.weight torch.Size([384, 192])\n",
      "blocks.5.proj.bias torch.Size([384])\n",
      "blocks.6.norm1.weight torch.Size([384])\n",
      "blocks.6.norm1.bias torch.Size([384])\n",
      "blocks.6.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.6.attn.qkv.bias torch.Size([1152])\n",
      "blocks.6.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.6.attn.proj.bias torch.Size([384])\n",
      "blocks.6.norm2.weight torch.Size([384])\n",
      "blocks.6.norm2.bias torch.Size([384])\n",
      "blocks.6.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.6.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.6.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.6.mlp.fc2.bias torch.Size([384])\n",
      "blocks.7.norm1.weight torch.Size([384])\n",
      "blocks.7.norm1.bias torch.Size([384])\n",
      "blocks.7.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.7.attn.qkv.bias torch.Size([1152])\n",
      "blocks.7.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.7.attn.proj.bias torch.Size([384])\n",
      "blocks.7.norm2.weight torch.Size([384])\n",
      "blocks.7.norm2.bias torch.Size([384])\n",
      "blocks.7.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.7.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.7.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.7.mlp.fc2.bias torch.Size([384])\n",
      "blocks.8.norm1.weight torch.Size([384])\n",
      "blocks.8.norm1.bias torch.Size([384])\n",
      "blocks.8.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.8.attn.qkv.bias torch.Size([1152])\n",
      "blocks.8.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.8.attn.proj.bias torch.Size([384])\n",
      "blocks.8.norm2.weight torch.Size([384])\n",
      "blocks.8.norm2.bias torch.Size([384])\n",
      "blocks.8.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.8.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.8.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.8.mlp.fc2.bias torch.Size([384])\n",
      "blocks.9.norm1.weight torch.Size([384])\n",
      "blocks.9.norm1.bias torch.Size([384])\n",
      "blocks.9.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.9.attn.qkv.bias torch.Size([1152])\n",
      "blocks.9.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.9.attn.proj.bias torch.Size([384])\n",
      "blocks.9.norm2.weight torch.Size([384])\n",
      "blocks.9.norm2.bias torch.Size([384])\n",
      "blocks.9.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.9.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.9.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.9.mlp.fc2.bias torch.Size([384])\n",
      "blocks.10.norm1.weight torch.Size([384])\n",
      "blocks.10.norm1.bias torch.Size([384])\n",
      "blocks.10.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.10.attn.qkv.bias torch.Size([1152])\n",
      "blocks.10.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.10.attn.proj.bias torch.Size([384])\n",
      "blocks.10.norm2.weight torch.Size([384])\n",
      "blocks.10.norm2.bias torch.Size([384])\n",
      "blocks.10.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.10.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.10.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.10.mlp.fc2.bias torch.Size([384])\n",
      "blocks.11.norm1.weight torch.Size([384])\n",
      "blocks.11.norm1.bias torch.Size([384])\n",
      "blocks.11.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.11.attn.qkv.bias torch.Size([1152])\n",
      "blocks.11.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.11.attn.proj.bias torch.Size([384])\n",
      "blocks.11.norm2.weight torch.Size([384])\n",
      "blocks.11.norm2.bias torch.Size([384])\n",
      "blocks.11.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.11.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.11.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.11.mlp.fc2.bias torch.Size([384])\n",
      "blocks.12.norm1.weight torch.Size([384])\n",
      "blocks.12.norm1.bias torch.Size([384])\n",
      "blocks.12.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.12.attn.qkv.bias torch.Size([1152])\n",
      "blocks.12.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.12.attn.proj.bias torch.Size([384])\n",
      "blocks.12.norm2.weight torch.Size([384])\n",
      "blocks.12.norm2.bias torch.Size([384])\n",
      "blocks.12.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.12.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.12.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.12.mlp.fc2.bias torch.Size([384])\n",
      "blocks.13.norm1.weight torch.Size([384])\n",
      "blocks.13.norm1.bias torch.Size([384])\n",
      "blocks.13.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.13.attn.qkv.bias torch.Size([1152])\n",
      "blocks.13.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.13.attn.proj.bias torch.Size([384])\n",
      "blocks.13.norm2.weight torch.Size([384])\n",
      "blocks.13.norm2.bias torch.Size([384])\n",
      "blocks.13.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.13.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.13.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.13.mlp.fc2.bias torch.Size([384])\n",
      "blocks.14.norm1.weight torch.Size([384])\n",
      "blocks.14.norm1.bias torch.Size([384])\n",
      "blocks.14.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.14.attn.qkv.bias torch.Size([1152])\n",
      "blocks.14.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.14.attn.proj.bias torch.Size([384])\n",
      "blocks.14.norm2.weight torch.Size([384])\n",
      "blocks.14.norm2.bias torch.Size([384])\n",
      "blocks.14.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.14.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.14.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.14.mlp.fc2.bias torch.Size([384])\n",
      "blocks.15.norm1.weight torch.Size([384])\n",
      "blocks.15.norm1.bias torch.Size([384])\n",
      "blocks.15.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.15.attn.qkv.bias torch.Size([1152])\n",
      "blocks.15.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.15.attn.proj.bias torch.Size([384])\n",
      "blocks.15.norm2.weight torch.Size([384])\n",
      "blocks.15.norm2.bias torch.Size([384])\n",
      "blocks.15.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.15.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.15.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.15.mlp.fc2.bias torch.Size([384])\n",
      "blocks.16.norm1.weight torch.Size([384])\n",
      "blocks.16.norm1.bias torch.Size([384])\n",
      "blocks.16.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.16.attn.qkv.bias torch.Size([1152])\n",
      "blocks.16.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.16.attn.proj.bias torch.Size([384])\n",
      "blocks.16.norm2.weight torch.Size([384])\n",
      "blocks.16.norm2.bias torch.Size([384])\n",
      "blocks.16.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.16.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.16.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.16.mlp.fc2.bias torch.Size([384])\n",
      "blocks.17.norm1.weight torch.Size([384])\n",
      "blocks.17.norm1.bias torch.Size([384])\n",
      "blocks.17.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.17.attn.qkv.bias torch.Size([1152])\n",
      "blocks.17.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.17.attn.proj.bias torch.Size([384])\n",
      "blocks.17.norm2.weight torch.Size([384])\n",
      "blocks.17.norm2.bias torch.Size([384])\n",
      "blocks.17.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.17.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.17.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.17.mlp.fc2.bias torch.Size([384])\n",
      "blocks.18.norm1.weight torch.Size([384])\n",
      "blocks.18.norm1.bias torch.Size([384])\n",
      "blocks.18.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.18.attn.qkv.bias torch.Size([1152])\n",
      "blocks.18.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.18.attn.proj.bias torch.Size([384])\n",
      "blocks.18.norm2.weight torch.Size([384])\n",
      "blocks.18.norm2.bias torch.Size([384])\n",
      "blocks.18.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.18.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.18.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.18.mlp.fc2.bias torch.Size([384])\n",
      "blocks.19.norm1.weight torch.Size([384])\n",
      "blocks.19.norm1.bias torch.Size([384])\n",
      "blocks.19.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.19.attn.qkv.bias torch.Size([1152])\n",
      "blocks.19.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.19.attn.proj.bias torch.Size([384])\n",
      "blocks.19.norm2.weight torch.Size([384])\n",
      "blocks.19.norm2.bias torch.Size([384])\n",
      "blocks.19.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.19.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.19.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.19.mlp.fc2.bias torch.Size([384])\n",
      "blocks.20.norm1.weight torch.Size([384])\n",
      "blocks.20.norm1.bias torch.Size([384])\n",
      "blocks.20.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.20.attn.qkv.bias torch.Size([1152])\n",
      "blocks.20.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.20.attn.proj.bias torch.Size([384])\n",
      "blocks.20.norm2.weight torch.Size([384])\n",
      "blocks.20.norm2.bias torch.Size([384])\n",
      "blocks.20.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.20.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.20.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.20.mlp.fc2.bias torch.Size([384])\n",
      "blocks.21.norm1.weight torch.Size([384])\n",
      "blocks.21.norm1.bias torch.Size([384])\n",
      "blocks.21.attn.qkv.weight torch.Size([2304, 384])\n",
      "blocks.21.attn.qkv.bias torch.Size([2304])\n",
      "blocks.21.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.21.attn.proj.bias torch.Size([768])\n",
      "blocks.21.norm2.weight torch.Size([768])\n",
      "blocks.21.norm2.bias torch.Size([768])\n",
      "blocks.21.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.21.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.21.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.21.mlp.fc2.bias torch.Size([768])\n",
      "blocks.21.proj.weight torch.Size([768, 384])\n",
      "blocks.21.proj.bias torch.Size([768])\n",
      "blocks.22.norm1.weight torch.Size([768])\n",
      "blocks.22.norm1.bias torch.Size([768])\n",
      "blocks.22.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.22.attn.qkv.bias torch.Size([2304])\n",
      "blocks.22.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.22.attn.proj.bias torch.Size([768])\n",
      "blocks.22.norm2.weight torch.Size([768])\n",
      "blocks.22.norm2.bias torch.Size([768])\n",
      "blocks.22.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.22.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.22.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.22.mlp.fc2.bias torch.Size([768])\n",
      "blocks.23.norm1.weight torch.Size([768])\n",
      "blocks.23.norm1.bias torch.Size([768])\n",
      "blocks.23.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.23.attn.qkv.bias torch.Size([2304])\n",
      "blocks.23.attn.proj.weight torch.Size([768, 768])\n",
      "blocks.23.attn.proj.bias torch.Size([768])\n",
      "blocks.23.norm2.weight torch.Size([768])\n",
      "blocks.23.norm2.bias torch.Size([768])\n",
      "blocks.23.mlp.fc1.weight torch.Size([3072, 768])\n",
      "blocks.23.mlp.fc1.bias torch.Size([3072])\n",
      "blocks.23.mlp.fc2.weight torch.Size([768, 3072])\n",
      "blocks.23.mlp.fc2.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "model_dict = checkpoint['model_state']\n",
    "print(model_dict['patch_embed.proj.weight'].shape)\n",
    "for k, v in model_dict.items():\n",
    "    if 'blocks' in k:\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68b538e9-5aae-40ff-b91f-59c53ade4724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tpeat3/.cache/torch/hub/facebookresearch_dino_main\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /home/tpeat3/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n",
      "100%|██████████| 82.7M/82.7M [00:03<00:00, 28.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54004223-b797-40cb-9698-7e600538a9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(vits16.state_dict().keys())\n",
    "\n",
    "for k, v in vits16.state_dict().items():\n",
    "    if 'blocks' in k:\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87a40b70-d404-46e8-8c79-07730c7ca3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.norm1.weight torch.Size([96])\n",
      "blocks.0.norm1.bias torch.Size([96])\n",
      "blocks.0.attn.qkv.weight torch.Size([288, 96])\n",
      "blocks.0.attn.qkv.bias torch.Size([288])\n",
      "blocks.0.attn.proj.weight torch.Size([96, 96])\n",
      "blocks.0.attn.proj.bias torch.Size([96])\n",
      "blocks.0.norm2.weight torch.Size([96])\n",
      "blocks.0.norm2.bias torch.Size([96])\n",
      "blocks.0.mlp.fc1.weight torch.Size([384, 96])\n",
      "blocks.0.mlp.fc1.bias torch.Size([384])\n",
      "blocks.0.mlp.fc2.weight torch.Size([96, 384])\n",
      "blocks.0.mlp.fc2.bias torch.Size([96])\n",
      "blocks.1.norm1.weight torch.Size([96])\n",
      "blocks.1.norm1.bias torch.Size([96])\n",
      "blocks.1.attn.qkv.weight torch.Size([288, 96])\n",
      "blocks.1.attn.qkv.bias torch.Size([288])\n",
      "blocks.1.attn.proj.weight torch.Size([96, 96])\n",
      "blocks.1.attn.proj.bias torch.Size([96])\n",
      "blocks.1.norm2.weight torch.Size([96])\n",
      "blocks.1.norm2.bias torch.Size([96])\n",
      "blocks.1.mlp.fc1.weight torch.Size([384, 96])\n",
      "blocks.1.mlp.fc1.bias torch.Size([384])\n",
      "blocks.1.mlp.fc2.weight torch.Size([96, 384])\n",
      "blocks.1.mlp.fc2.bias torch.Size([96])\n",
      "blocks.2.norm1.weight torch.Size([96])\n",
      "blocks.2.norm1.bias torch.Size([96])\n",
      "blocks.2.attn.qkv.weight torch.Size([288, 96])\n",
      "blocks.2.attn.qkv.bias torch.Size([288])\n",
      "blocks.2.attn.proj.weight torch.Size([96, 96])\n",
      "blocks.2.attn.proj.bias torch.Size([96])\n",
      "blocks.2.norm2.weight torch.Size([96])\n",
      "blocks.2.norm2.bias torch.Size([96])\n",
      "blocks.2.mlp.fc1.weight torch.Size([384, 96])\n",
      "blocks.2.mlp.fc1.bias torch.Size([384])\n",
      "blocks.2.mlp.fc2.weight torch.Size([96, 384])\n",
      "blocks.2.mlp.fc2.bias torch.Size([96])\n",
      "blocks.3.norm1.weight torch.Size([192])\n",
      "blocks.3.norm1.bias torch.Size([192])\n",
      "blocks.3.attn.qkv.weight torch.Size([576, 192])\n",
      "blocks.3.attn.qkv.bias torch.Size([576])\n",
      "blocks.3.attn.proj.weight torch.Size([192, 192])\n",
      "blocks.3.attn.proj.bias torch.Size([192])\n",
      "blocks.3.norm2.weight torch.Size([192])\n",
      "blocks.3.norm2.bias torch.Size([192])\n",
      "blocks.3.mlp.fc1.weight torch.Size([768, 192])\n",
      "blocks.3.mlp.fc1.bias torch.Size([768])\n",
      "blocks.3.mlp.fc2.weight torch.Size([192, 768])\n",
      "blocks.3.mlp.fc2.bias torch.Size([192])\n",
      "blocks.4.norm1.weight torch.Size([192])\n",
      "blocks.4.norm1.bias torch.Size([192])\n",
      "blocks.4.attn.qkv.weight torch.Size([576, 192])\n",
      "blocks.4.attn.qkv.bias torch.Size([576])\n",
      "blocks.4.attn.proj.weight torch.Size([192, 192])\n",
      "blocks.4.attn.proj.bias torch.Size([192])\n",
      "blocks.4.norm2.weight torch.Size([192])\n",
      "blocks.4.norm2.bias torch.Size([192])\n",
      "blocks.4.mlp.fc1.weight torch.Size([768, 192])\n",
      "blocks.4.mlp.fc1.bias torch.Size([768])\n",
      "blocks.4.mlp.fc2.weight torch.Size([192, 768])\n",
      "blocks.4.mlp.fc2.bias torch.Size([192])\n",
      "blocks.5.norm1.weight torch.Size([192])\n",
      "blocks.5.norm1.bias torch.Size([192])\n",
      "blocks.5.attn.qkv.weight torch.Size([576, 192])\n",
      "blocks.5.attn.qkv.bias torch.Size([576])\n",
      "blocks.5.attn.proj.weight torch.Size([192, 192])\n",
      "blocks.5.attn.proj.bias torch.Size([192])\n",
      "blocks.5.norm2.weight torch.Size([192])\n",
      "blocks.5.norm2.bias torch.Size([192])\n",
      "blocks.5.mlp.fc1.weight torch.Size([768, 192])\n",
      "blocks.5.mlp.fc1.bias torch.Size([768])\n",
      "blocks.5.mlp.fc2.weight torch.Size([192, 768])\n",
      "blocks.5.mlp.fc2.bias torch.Size([192])\n",
      "blocks.6.norm1.weight torch.Size([384])\n",
      "blocks.6.norm1.bias torch.Size([384])\n",
      "blocks.6.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.6.attn.qkv.bias torch.Size([1152])\n",
      "blocks.6.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.6.attn.proj.bias torch.Size([384])\n",
      "blocks.6.norm2.weight torch.Size([384])\n",
      "blocks.6.norm2.bias torch.Size([384])\n",
      "blocks.6.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.6.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.6.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.6.mlp.fc2.bias torch.Size([384])\n",
      "blocks.7.norm1.weight torch.Size([384])\n",
      "blocks.7.norm1.bias torch.Size([384])\n",
      "blocks.7.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.7.attn.qkv.bias torch.Size([1152])\n",
      "blocks.7.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.7.attn.proj.bias torch.Size([384])\n",
      "blocks.7.norm2.weight torch.Size([384])\n",
      "blocks.7.norm2.bias torch.Size([384])\n",
      "blocks.7.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.7.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.7.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.7.mlp.fc2.bias torch.Size([384])\n",
      "blocks.8.norm1.weight torch.Size([384])\n",
      "blocks.8.norm1.bias torch.Size([384])\n",
      "blocks.8.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.8.attn.qkv.bias torch.Size([1152])\n",
      "blocks.8.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.8.attn.proj.bias torch.Size([384])\n",
      "blocks.8.norm2.weight torch.Size([384])\n",
      "blocks.8.norm2.bias torch.Size([384])\n",
      "blocks.8.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.8.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.8.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.8.mlp.fc2.bias torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.state_dict().items():\n",
    "    if 'blocks' in k and 'decoder' not in k:\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aba37c-b282-4032-b4a2-2abd717bf43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_network(model, 'pretrain_models/dino_deitsmall16_pretrain.pth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156d073-4d79-4013-bf79-91831be3d2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a8899-272c-46c2-bdab-1a79af7ee256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shortcuts = model.get_intermediates()\n",
    "for i in shortcuts:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1cfacb-3ae1-48af-813e-28dbb97f7d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL Lib",
   "language": "python",
   "name": "rllib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
